model:
  model_name: "gpt2"
  peft_checkpoint: "checkpoint-5"
  model_config:
  quant_config:
    load_in_4bit: true
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
    bnb_4bit_compute_dtype: "torch.bfloat16"
  from_peft: true
task_type: "CAUSAL_LM"
peft:
  type: "lora"
  peft_config:
    lora_alpha: 16
    lora_dropout: 0.1
    r: 2
    bias: "none"
    target_modules:
     - "c_attn"
data:
  data_location: "local"
  train_data: "clm_train.csv"
  eval_data:
token:
  max_length: 
  truncation: false
  padding: false
train:
  type: "trainer"
  final_save_path: "./"
  train_config:
    per_device_train_batch_size: 1
    learning_rate: 0.0001
    gradient_accumulation_steps: 4
    save_steps: 1
    logging_steps: 1
    max_grad_norm: 1
    num_train_epochs: 5
    warmup_ratio: 0.03
    lr_scheduler_type: "constant"
    report_to: 
    disable_tqdm: true
    group_by_length: true
    fp16: true
    optim: "adamw_bnb_8bit"



