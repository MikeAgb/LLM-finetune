access_token: {your access token here}
model:
  model_name: "meta-llama/Llama-2-7b-chat-hf"
  model_config:
  quant_config:
    load_in_4bit: true
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
    bnb_4bit_compute_dtype: "torch.bfloat16"
task_type: "CAUSAL_LM"
peft:
  type: "lora"
  peft_config:
    lora_alpha: 16
    lora_dropout: 0.05
    r: 8
    bias: "none"
    target_modules:
         - "q_proj"
         - "v_proj"
data:
  data_location: "local"
  train_data: "test.csv"
  eval_data:
token:
  max_length:
  truncation: false
  padding: false
train:
  type: "trainer"
  final_save_path: "./llama7B"
  train_config:
    per_device_train_batch_size: 1
    learning_rate: 0.0003
    gradient_accumulation_steps: 4
    save_steps: 1
    logging_steps: 1
    max_grad_norm: 1
    num_train_epochs: 5
    warmup_ratio: 0.03
    lr_scheduler_type: "constant"
    report_to:
    disable_tqdm: true
    group_by_length: true
    fp16: true
    optim: "adamw_bnb_8bit"